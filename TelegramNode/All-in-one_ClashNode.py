"""
å›ºå®šé“¾æ¥è·å–èŠ‚ç‚¹è„šæœ¬ V2
-----------------------------------------
åŠŸèƒ½è¯´æ˜ï¼š
æœ¬è„šæœ¬ç”¨äºä» URL.TXT æ–‡ä»¶ä¸­è¯»å–å¤šä¸ªä»¥â€œ#â€å¼€å¤´åˆ’åˆ†çš„è®¢é˜…åŒºå—ï¼Œæ¯ä¸ªåŒºå—åŒ…å«è‹¥å¹²è®¢é˜…é“¾æ¥ã€‚
è„šæœ¬ä¼šï¼š
1. è‡ªåŠ¨è¯†åˆ«å¹¶æ‹†åˆ†å¤šä¸ªè®¢é˜…åŒºå—ï¼›
2. é’ˆå¯¹æ¯ä¸ªåŒºå—ï¼Œæå–æ‰€æœ‰ HTTP/HTTPS è®¢é˜…é“¾æ¥ï¼›
3. ä¾æ¬¡ä¸‹è½½è®¢é˜…å†…å®¹ï¼ˆä¼˜å…ˆä½¿ç”¨ wgetï¼Œå¤±è´¥åä½¿ç”¨ requestsï¼‰ï¼›
4. V2.è‡ªåŠ¨è¯†åˆ« YAML ç›´æ¥è§£æã€å†æ˜æ–‡åˆ¤å®šè§£æã€å Base64 è§£ç èŠ‚ç‚¹è§£æï¼ˆvmessã€vlessã€ssrã€ssã€trojanã€hysteriaç­‰ï¼‰ï¼›
5. åˆå¹¶å»é‡æ‰€æœ‰èŠ‚ç‚¹ï¼ŒåŒæ—¶æ”¯æŒèŠ‚ç‚¹æµ‹é€Ÿï¼ˆé€šè¿‡çº¯ Python socketï¼‰ç­›é€‰å¯ç”¨èŠ‚ç‚¹ï¼›
6. æ™ºèƒ½ä¸ºæ‰€æœ‰èŠ‚ç‚¹æ·»åŠ ç¬¦åˆè§„åˆ™çš„åŒºåŸŸæ ‡è¯†å’Œå›½æ—— Emojiï¼Œå¹¶é‡å‘½åï¼›
7. æŒ‰åœ°åŒºä¼˜å…ˆçº§åŠæµ‹é€Ÿç»“æœæ’åºèŠ‚ç‚¹ï¼›
8. ä¸ºæ¯ä¸ªåŒºå—ç”Ÿæˆç‹¬ç«‹çš„ Clash é…ç½® YAML æ–‡ä»¶ï¼Œæ–‡ä»¶ä¿å­˜åœ¨ output_yaml ç›®å½•ä¸­ã€‚
ä½¿ç”¨è¯´æ˜ï¼š
- åœ¨ URL.TXT ä¸­æ·»åŠ è®¢é˜…ï¼Œä½¿ç”¨â€œ# åŒºå—åç§°:â€æ ¼å¼åˆ’åˆ†å¤šä¸ªåŒºå—ï¼Œæ¯å—ä¸‹æ–¹ä¸ºç›¸å…³è®¢é˜…é“¾æ¥åˆ—è¡¨
- è¿è¡Œè„šæœ¬ï¼Œå³å¯åœ¨ output_yaml ç›®å½•ä¸­å¾—åˆ°åˆ†å—ç”Ÿæˆçš„ YAML é…ç½®æ–‡ä»¶
"""
import os
import re
import yaml
import base64
import json
import socket
import shutil
import hashlib
import subprocess
import requests
import concurrent.futures
import time
from datetime import datetime, timedelta, timezone
from collections import defaultdict
from urllib.parse import urlparse, parse_qs, unquote

# ========== åŸºç¡€é…ç½® ==========
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__)) # è·å–å½“å‰è„šæœ¬æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•çš„ç»å¯¹è·¯å¾„
URL_FILE = os.path.join(SCRIPT_DIR, "URL.TXT") # æ„å»ºURLæ–‡ä»¶çš„å®Œæ•´è·¯å¾„ï¼Œè¯¥æ–‡ä»¶åº”ä½äºè„šæœ¬åŒç›®å½•ä¸‹
OUTPUT_DIR = os.path.join(SCRIPT_DIR, "output_yaml") # æ„å»ºè¾“å‡ºç›®å½•çš„å®Œæ•´è·¯å¾„ï¼Œç”¨äºå­˜æ”¾ç”Ÿæˆçš„YAMLæ–‡ä»¶
os.makedirs(OUTPUT_DIR, exist_ok=True) # åˆ›å»ºè¾“å‡ºç›®å½•ï¼Œå¦‚æœç›®å½•å·²å­˜åœ¨åˆ™ä¸æŠ¥é”™

# ========== æµ‹é€Ÿé…ç½® ========== # ä»¥ä¸‹æ˜¯å…³äºé€Ÿåº¦æµ‹è¯•çš„é…ç½®é¡¹
ENABLE_SPEED_TEST = True # æ˜¯å¦å¯ç”¨é€Ÿåº¦æµ‹è¯•ï¼Œè®¾ç½®ä¸ºTrueè¡¨ç¤ºå¯ç”¨
SOCKET_TIMEOUT = 10 # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
MAX_TEST_WORKERS = 256 # æœ€å¤§æµ‹è¯•å·¥ä½œçº¿ç¨‹æ•°ï¼Œè¡¨ç¤ºåŒæ—¶è¿›è¡Œé€Ÿåº¦æµ‹è¯•çš„æœ€å¤§å¹¶å‘è¿æ¥æ•°

# ========== åŒºåŸŸæ˜ å°„ä¸è§„åˆ™ï¼ˆåˆå¹¶ç‰ˆï¼‰ ==========
REGION_PRIORITY = ['é¦™æ¸¯', 'æ—¥æœ¬', 'æ–°åŠ å¡', 'ç¾å›½', 'å°æ¹¾', 'éŸ©å›½', 'å¾·å›½', 'è‹±å›½', 'åŠ æ‹¿å¤§', 'æ¾³å¤§åˆ©äºš']

CHINESE_COUNTRY_MAP = {
    'US': 'ç¾å›½', 'United States': 'ç¾å›½', 'USA': 'ç¾å›½', 'America': 'ç¾å›½',
    'New York': 'ç¾å›½', 'Los Angeles': 'ç¾å›½', 'Washington': 'ç¾å›½', 'Chicago': 'ç¾å›½',
    'San Francisco': 'ç¾å›½', 'Las Vegas': 'ç¾å›½', 'Miami': 'ç¾å›½', 'Seattle': 'ç¾å›½',
    'Houston': 'ç¾å›½', 'Boston': 'ç¾å›½', 'Atlanta': 'ç¾å›½', 'Dallas': 'ç¾å›½',

    'JP': 'æ—¥æœ¬', 'Japan': 'æ—¥æœ¬', 'Tokyo': 'æ—¥æœ¬', 'Osaka': 'æ—¥æœ¬', 'Nagoya': 'æ—¥æœ¬',
    'Sapporo': 'æ—¥æœ¬', 'Fukuoka': 'æ—¥æœ¬', 'NTT': 'æ—¥æœ¬', 'IIJ': 'æ—¥æœ¬', 'GMO': 'æ—¥æœ¬', 'Linode': 'æ—¥æœ¬',

    'HK': 'é¦™æ¸¯', 'Hong Kong': 'é¦™æ¸¯', 'HongKong': 'é¦™æ¸¯', 'HKT': 'é¦™æ¸¯',
    'ä¹é¾™': 'é¦™æ¸¯', 'æ²™ç”°': 'é¦™æ¸¯', 'å±¯é—¨': 'é¦™æ¸¯', 'èƒæ¹¾': 'é¦™æ¸¯', 'æ·±æ°´åŸ—': 'é¦™æ¸¯', 'æ²¹å°–æ—º': 'é¦™æ¸¯',

    'SG': 'æ–°åŠ å¡', 'Singapore': 'æ–°åŠ å¡', 'SGP': 'æ–°åŠ å¡', 'SG': 'æ–°åŠ å¡',
    'æ˜Ÿ': 'æ–°åŠ å¡', 'ç‹®åŸ': 'æ–°åŠ å¡', 'å¡': 'æ–°åŠ å¡',

    'TW': 'å°æ¹¾', 'Taiwan': 'å°æ¹¾', 'TWN': 'å°æ¹¾',
    'Taipei': 'å°æ¹¾', 'Taichung': 'å°æ¹¾', 'Kaohsiung': 'å°æ¹¾',
    'æ–°åŒ—': 'å°æ¹¾', 'å½°åŒ–': 'å°æ¹¾', 'Hinet': 'å°æ¹¾', 'ä¸­åç”µä¿¡': 'å°æ¹¾',

    'KR': 'éŸ©å›½', 'Korea': 'éŸ©å›½', 'KOR': 'éŸ©å›½', 'Seoul': 'éŸ©å›½',
    'Busan': 'éŸ©å›½', 'KT': 'éŸ©å›½', 'SK': 'éŸ©å›½', 'LG': 'éŸ©å›½',
    'å—æœé²œ': 'éŸ©å›½', 'éŸ©': 'éŸ©å›½', 'éŸ“': 'éŸ©å›½',

    'DE': 'å¾·å›½', 'Germany': 'å¾·å›½', 'Frankfurt': 'å¾·å›½',
    'Munich': 'å¾·å›½', 'Berlin': 'å¾·å›½', 'Hetzner': 'å¾·å›½',

    'GB': 'è‹±å›½', 'United Kingdom': 'è‹±å›½', 'UK': 'è‹±å›½',
    'England': 'è‹±å›½', 'London': 'è‹±å›½', 'Manchester': 'è‹±å›½',

    'CA': 'åŠ æ‹¿å¤§', 'Canada': 'åŠ æ‹¿å¤§', 'Toronto': 'åŠ æ‹¿å¤§',
    'Vancouver': 'åŠ æ‹¿å¤§', 'Montreal': 'åŠ æ‹¿å¤§',

    'AU': 'æ¾³å¤§åˆ©äºš', 'Australia': 'æ¾³å¤§åˆ©äºš',
    'Sydney': 'æ¾³å¤§åˆ©äºš', 'Melbourne': 'æ¾³å¤§åˆ©äºš', 'Brisbane': 'æ¾³å¤§åˆ©äºš',
}


COUNTRY_NAME_TO_CODE_MAP = {
"é˜¿å¯Œæ±—":"AF", "é˜¿å°”å·´å°¼äºš":"AL", "é˜¿å°”åŠåˆ©äºš":"DZ", "å®‰é“å°”":"AD", "å®‰å“¥æ‹‰":"AO", "å®‰åœ­æ‹‰":"AI", 
"å®‰æç“œå’Œå·´å¸ƒè¾¾":"AG", "é˜¿æ ¹å»·":"AR", "äºšç¾å°¼äºš":"AM", "é˜¿é²å·´":"AW", "æ¾³å¤§åˆ©äºš":"AU", "å¥¥åœ°åˆ©":"AT", "é˜¿å¡æ‹œç–†":"AZ", "å·´å“ˆé©¬":"BS", 
"å·´æ—":"BH", "å­ŸåŠ æ‹‰å›½":"BD", "å·´å·´å¤šæ–¯":"BB", "ç™½ä¿„ç½—æ–¯":"BY", "æ¯”åˆ©æ—¶":"BE", "ä¼¯åˆ©å…¹":"BZ", "è´å®":"BJ", "ç™¾æ…•å¤§":"BM", "ä¸ä¸¹":"BT", 
"ç»åˆ©ç»´äºš":"BO", "æ³¢é»‘":"BA", "åšèŒ¨ç“¦çº³":"BW", "å·´è¥¿":"BR", "æ–‡è±":"BN", "ä¿åŠ åˆ©äºš":"BG", "å¸ƒåŸºçº³æ³•ç´¢":"BF", "å¸ƒéš†è¿ª":"BI", "æŸ¬åŸ”å¯¨":"KH", 
"å–€éº¦éš†":"CM", "åŠ æ‹¿å¤§":"CA", "ä½›å¾—è§’":"CV", "å¼€æ›¼ç¾¤å²›":"KY", "ä¸­é":"CF", "ä¹å¾—":"TD", "æ™ºåˆ©":"CL", "ä¸­å›½":"CN", "å“¥ä¼¦æ¯”äºš":"CO", 
"ç§‘æ‘©ç½—":"KM", "åˆšæœï¼ˆé‡‘ï¼‰":"CD", "åˆšæœï¼ˆå¸ƒï¼‰":"CG", "å“¥æ–¯è¾¾é»åŠ ":"CR", "ç§‘ç‰¹è¿ªç“¦":"CI", "å…‹ç½—åœ°äºš":"HR", "å¤å·´":"CU", "å¡æµ¦è·¯æ–¯":"CY", 
"æ·å…‹":"CZ", "ä¸¹éº¦":"DK", "å‰å¸ƒæ":"DJ", "å¤šç±³å°¼å…‹":"DM", "å¤šç±³å°¼åŠ ":"DO", "å„ç“œå¤šå°”":"EC", "åŸƒåŠ":"EG", "è¨å°”ç“¦å¤š":"SV", "èµ¤é“å‡ å†…äºš":"GQ", 
"å„ç«‹ç‰¹é‡Œäºš":"ER", "çˆ±æ²™å°¼äºš":"EE", "åŸƒå¡ä¿„æ¯”äºš":"ET", "æ–æµ":"FJ", "èŠ¬å…°":"FI", "æ³•å›½":"FR", "åŠ è“¬":"GA", "å†ˆæ¯”äºš":"GM", "æ ¼é²å‰äºš":"GE", 
"åŠ çº³":"GH", "å¸Œè…Š":"GR", "æ ¼æ—çº³è¾¾":"GD", "å±åœ°é©¬æ‹‰":"GT", "å‡ å†…äºš":"GN", "å‡ å†…äºšæ¯”ç»":"GW", "åœ­äºšé‚£":"GY", "æµ·åœ°":"HT", "æ´ªéƒ½æ‹‰æ–¯":"HN", 
"åŒˆç‰™åˆ©":"HU", "å†°å²›":"IS", "å°åº¦":"IN", "å°å°¼":"ID", "å°åº¦å°¼è¥¿äºš":"ID", "ä¼Šæœ—":"IR", "ä¼Šæ‹‰å…‹":"IQ", "çˆ±å°”å…°":"IE", "ä»¥è‰²åˆ—":"IL", 
"æ„å¤§åˆ©":"IT", "ç‰™ä¹°åŠ ":"JM", "æ—¥æœ¬":"JP", "çº¦æ—¦":"JO", "å“ˆè¨å…‹æ–¯å¦":"KZ", "è‚¯å°¼äºš":"KE", "åŸºé‡Œå·´æ–¯":"KI", "ç§‘å¨ç‰¹":"KW", 
"å‰å°”å‰æ–¯æ–¯å¦":"KG", "è€æŒ":"LA", "æ‹‰è„±ç»´äºš":"LV", "é»å·´å«©":"LB", "è±ç´¢æ‰˜":"LS", "åˆ©æ¯”é‡Œäºš":"LR", "åˆ©æ¯”äºš":"LY", "åˆ—æ”¯æ•¦å£«ç™»":"LI", 
"ç«‹é™¶å®›":"LT", "å¢æ£®å ¡":"LU", "æ¾³é—¨":"MO", "åŒ—é©¬å…¶é¡¿":"MK", "é©¬è¾¾åŠ æ–¯åŠ ":"MG", "é©¬æ‹‰ç»´":"MW", "é©¬æ¥è¥¿äºš":"MY", "é©¬å°”ä»£å¤«":"MV", "é©¬é‡Œ":"ML", 
"é©¬è€³ä»–":"MT", "é©¬ç»å°”ç¾¤å²›":"MH", "æ¯›é‡Œå¡”å°¼äºš":"MR", "æ¯›é‡Œæ±‚æ–¯":"MU", "å¢¨è¥¿å“¥":"MX", "å¯†å…‹ç½—å°¼è¥¿äºš":"FM", "æ‘©å°”å¤šç“¦":"MD", "æ‘©çº³å“¥":"MC", 
"è’™å¤":"MN", "é»‘å±±":"ME", "æ‘©æ´›å“¥":"MA", "è«æ¡‘æ¯”å…‹":"MZ", "ç¼…ç”¸":"MM", "çº³ç±³æ¯”äºš":"NA", "ç‘™é²":"NR", "å°¼æ³Šå°”":"NP", "è·å…°":"NL", "æ–°è¥¿å…°":"NZ", 
"å°¼åŠ æ‹‰ç“œ":"NI", "å°¼æ—¥å°”":"NE", "å°¼æ—¥åˆ©äºš":"NG", "æŒªå¨":"NO", "é˜¿æ›¼":"OM", "å·´åŸºæ–¯å¦":"PK", "å¸•åŠ³":"PW", "å·´å‹’æ–¯å¦":"PS", "å·´æ‹¿é©¬":"PA", 
"å·´å¸ƒäºšæ–°å‡ å†…äºš":"PG", "å·´æ‹‰åœ­":"PY", "ç§˜é²":"PE", "è²å¾‹å®¾":"PH", "æ³¢å…°":"PL", "è‘¡è„ç‰™":"PT", "å¡å¡”å°”":"QA", "ç½—é©¬å°¼äºš":"RO", "ä¿„ç½—æ–¯":"RU", 
"å¢æ—ºè¾¾":"RW", "åœ£é©¬åŠ›è¯º":"SM", "æ²™ç‰¹é˜¿æ‹‰ä¼¯":"SA", "å¡å†…åŠ å°”":"SN", "å¡å°”ç»´äºš":"RS", "å¡èˆŒå°”":"SC", "å¡æ‹‰åˆ©æ˜‚":"SL", "æ–°åŠ å¡":"SG", "æ–¯æ´›ä¼å…‹":"SK", 
"æ–¯æ´›æ–‡å°¼äºš":"SI", "æ‰€ç½—é—¨ç¾¤å²›":"SB", "ç´¢é©¬é‡Œ":"SO", "å—é":"ZA", "è¥¿ç­ç‰™":"ES", "æ–¯é‡Œå…°å¡":"LK", "è‹ä¸¹":"SD", "è‹é‡Œå—":"SR", "ç‘å…¸":"SE", 
"ç‘å£«":"CH", "å™åˆ©äºš":"SY", "å¡”å‰å…‹æ–¯å¦":"TJ", "å¦æ¡‘å°¼äºš":"TZ", "æ³°å›½":"TH", "ä¸œå¸æ±¶":"TL", "å¤šå“¥":"TG", "æ±¤åŠ ":"TO", "ç‰¹ç«‹å°¼è¾¾å’Œå¤šå·´å“¥":"TT", 
"çªå°¼æ–¯":"TN", "åœŸè€³å…¶":"TR", "åœŸåº“æ›¼æ–¯å¦":"TM", "å›¾ç“¦å¢":"TV", "ä¹Œå¹²è¾¾":"UG", "ä¹Œå…‹å…°":"UA", "é˜¿è”é…‹":"AE", "ä¹Œæ‹‰åœ­":"UY", "ä¹Œå…¹åˆ«å…‹æ–¯å¦":"UZ", 
"ç“¦åŠªé˜¿å›¾":"VU", "å§”å†…ç‘æ‹‰":"VE", "è¶Šå—":"VN", "ä¹Ÿé—¨":"YE", "èµæ¯”äºš":"ZM", "æ´¥å·´å¸ƒéŸ¦":"ZW"
}


JUNK_PATTERNS = re.compile(
    r"(?:ä¸“çº¿|IPLC|IEPL|BGP|ä½“éªŒ|ä¸‘å›¢|å®˜ç½‘|å€ç‡|x\d[\.\d]*|Rate|[\[\(ã€ã€Œ].*?[\]\)ã€‘ã€]|^\s*@\w+\s*|Relay|æµé‡)"
    r"|(?:(?:[\u2460-\u2473\u2776-\u277F\u2780-\u2789]|å…è²»|å›å®¶).*?(?=,|$))",
    re.IGNORECASE
)

CUSTOM_REGEX_RULES = {
    'é¦™æ¸¯': {
        'code': 'HK',
        'pattern': r'é¦™æ¸¯|æ¸¯|HK|Hong\s*Kong|HongKong|HKBN|HGC|PCCW|WTT|HKT|ä¹é¾™|æ²™ç”°|å±¯é—¨|èƒæ¹¾|æ·±æ°´åŸ—|æ²¹å°–æ—º'
    },
    'æ—¥æœ¬': {
        'code': 'JP',
        'pattern': r'æ—¥æœ¬|æ—¥|å·æ—¥|ä¸œäº¬|å¤§é˜ª|æ³‰æ—¥|æ²ªæ—¥|æ·±æ—¥|äº¬æ—¥|å¹¿æ—¥|JP|Japan|Tokyo|Osaka|Saitama|åŸ¼ç‰|åå¤å±‹|Nagoya|ç¦å†ˆ|Fukuoka|æ¨ªæ»¨|Yokohama|NTT|IIJ|GMO|Linode'
    },
    'æ–°åŠ å¡': {
        'code': 'SG',
        'pattern': r'æ–°åŠ å¡|å¡|ç‹®åŸ|ç‹®|æ–°|SG|Singapore|SG\d+|SGP|æ˜Ÿ|ç‹®å­åŸ'
    },
    'ç¾å›½': {
        'code': 'US',
        'pattern': r'ç¾å›½|ç¾|æ³¢ç‰¹å…°|è¾¾æ‹‰æ–¯|Oregon|ä¿„å‹’å†ˆ|å‡¤å‡°åŸ|ç¡…è°·|æ‹‰æ–¯ç»´åŠ æ–¯|æ´›æ‰çŸ¶|åœ£ä½•å¡|è¥¿é›…å›¾|èŠåŠ å“¥|çº½çº¦|è¿ˆé˜¿å¯†|äºšç‰¹å…°å¤§|US|USA|United\s*States|America|LA|NYC|SF|San\s*Francisco|Washington|åç››é¡¿|Kansas|å ªè¨æ–¯|Denver|ä¸¹ä½›|Phoenix|Seattle|Chicago|Boston|æ³¢å£«é¡¿|Atlanta|Miami|Las\s*Vegas'
    },
    'å°æ¹¾': {
        'code': 'TW',
        'pattern': r'å°æ¹¾|æ¹¾çœ|å°|TW|Taiwan|TWN|å°åŒ—|Taipei|å°ä¸­|Taichung|é«˜é›„|Kaohsiung|æ–°åŒ—|å½°åŒ–|Hinet|ä¸­åç”µä¿¡'
    },
    'éŸ©å›½': {
        'code': 'KR',
        'pattern': r'éŸ©å›½|éŸ©|å—æœé²œ|é¦–å°”|é‡œå±±|ä»å·|KR|Korea|KOR|éŸ“|Seoul|Busan|KT|SK|LG'
    },
    'å¾·å›½': {
        'code': 'DE',
        'pattern': r'å¾·å›½|å¾·|æ³•å…°å…‹ç¦|æ…•å°¼é»‘|æŸæ—|DE|Germany|Frankfurt|Munich|Berlin|Hetzner'
    },
    'è‹±å›½': {
        'code': 'GB',
        'pattern': r'è‹±å›½|è‹±|ä¼¦æ•¦|æ›¼å½»æ–¯ç‰¹|UK|GB|United\s*Kingdom|Britain|England|London|Manchester'
    },
    'åŠ æ‹¿å¤§': {'code': 'CA', 'pattern': r'åŠ æ‹¿å¤§|æ«å¶|å¤šä¼¦å¤š|æ¸©å“¥å|è’™ç‰¹åˆ©å°”|CA|Canada'},
    'æ¾³å¤§åˆ©äºš': {'code': 'AU', 'pattern': r'æ¾³å¤§åˆ©äºš|æ¾³æ´²|æ‚‰å°¼|AU|Australia'},
}


FLAG_EMOJI_PATTERN = re.compile(r'[\U0001F1E6-\U0001F1FF]{2}')

def preprocess_regex_rules():
    for region, rules in CUSTOM_REGEX_RULES.items():
        parts = rules['pattern'].split('|')
        sorted_parts = sorted(parts, key=len, reverse=True)
        escaped_parts = [re.escape(p) for p in sorted_parts]
        CUSTOM_REGEX_RULES[region]['pattern'] = '|'.join(escaped_parts)

preprocess_regex_rules()

def sanitize_filename(name: str) -> str:
    import re
    match = re.match(r"#\s*(.*?)\s*[:ï¼š]", name, re.IGNORECASE)
    if match:
        title = match.group(1)
    else:
        title = name.lstrip('#').strip()
        title = re.sub(r'[:ï¼š]+$', '', title).strip()
    title = re.sub(r'[\\/:*?"<>|\sï¼š]+', '_', title)
    title = title.strip('_')
    return title or 'default'

def get_country_flag_emoji(country_code):
    if not country_code or len(country_code) != 2:
        return "â“"
    return "".join(chr(0x1F1E6 + ord(c.upper()) - ord('A')) for c in country_code)

def safe_b64decode(data):
    data = data.encode() if isinstance(data, str) else data
    missing_padding = (-len(data)) % 4
    data += b'=' * missing_padding
    return base64.urlsafe_b64decode(data)

# ----- ä¸‹è½½ç›¸å…³ -----
def attempt_download_using_wget(url):
    print(f"  â¬‡ï¸ wget ä¸‹è½½: {url[:80]}")
    if not shutil.which("wget"):
        print("  âœ— æœªå®‰è£… wget")
        return None
    try:
        proc = subprocess.run(
            ["wget", "-O", "-", "--timeout=30", "--header=User-Agent: Clash", url],
            capture_output=True, text=True, check=True, encoding='utf-8', errors='ignore'
        )
        return proc.stdout if proc.stdout else None
    except subprocess.CalledProcessError as e:
        print(f"  âœ— wget å¤±è´¥: {e.stderr.strip()}")
        return None

def attempt_download_using_requests(url):
    print(f"  â¬‡ï¸ requests ä¸‹è½½: {url[:80]}")
    try:
        headers = {'User-Agent': 'Clash'}
        r = requests.get(url, headers=headers, timeout=30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding or 'utf-8'
        return r.text
    except Exception as e:
        print(f"  âœ— requests å¤±è´¥: {e}")
        return None

# ----- æ–°å¢å‡½æ•°ï¼šè§£ææ˜æ–‡åè®®èŠ‚ç‚¹ -----
def parse_plain_nodes_from_text(text):
    proxies = []
    success_count = defaultdict(int)
    failure_count = defaultdict(int)
    for line in text.splitlines():
        line = line.strip()
        if not line:
            continue
        proxy = None
        proto = None
        if line.startswith('vmess://'):
            proto = 'vmess'
            proxy = parse_vmess_node(line)
        elif line.startswith('vless://'):
            proto = 'vless'
            proxy = parse_vless_node(line)
        elif line.startswith('ssr://'):
            proto = 'ssr'
            proxy = parse_ssr_node(line)
        elif line.startswith('ss://'):
            proto = 'ss'
            proxy = parse_ss_node(line)
        elif line.startswith('trojan://'):
            proto = 'trojan'
            proxy = parse_trojan_node(line)
        elif line.startswith('hysteria://'):
            proto = 'hysteria'
            proxy = parse_hysteria_node(line)
        elif line.startswith('hysteria2://'):
            proto = 'hysteria2'
            proxy = parse_hysteria2_node(line)
        else:
            continue

        if proxy:
            proxies.append(proxy)
            success_count[proto] += 1
        else:
            failure_count[proto] += 1

    for proto, count in success_count.items():
        print(f"  - æ˜æ–‡åè®®è§£æå®Œæˆï¼Œ{proto} èŠ‚ç‚¹æˆåŠŸæ•°ï¼š{count}")
    for proto, count in failure_count.items():
        print(f"  - æ˜æ–‡åè®®è§£æå¤±è´¥ï¼Œ{proto} èŠ‚ç‚¹å¤±è´¥æ•°ï¼š{count}")

    return proxies

# ----- ä¿®æ”¹ download_subscription å‡½æ•° -----
def download_subscription(url):
    content = attempt_download_using_wget(url)
    if content is None:
        content = attempt_download_using_requests(url)
    if content is None:
        return []

    # å…ˆå°è¯•yamlæ ¼å¼ç›´æ¥è§£æ
    proxies = parse_proxies_from_content(content)
    if proxies:
        return proxies

    # å°è¯•ä»æ˜æ–‡åè®®é“¾æ¥ä¸­æå–èŠ‚ç‚¹
    proxies = parse_plain_nodes_from_text(content)
    if proxies:
        return proxies

    # å†å°è¯•base64ç¼–ç è§£ç å¹¶è§£æ
    if is_base64(content):
        proxies = decode_base64_and_parse(content)
        if proxies:
            return proxies
        print("  - Base64 è§£ç æ— æ•ˆèŠ‚ç‚¹")
    else:
        print("  - å†…å®¹é Base64ï¼Œä¸”æœªåŒ¹é…åˆ°æ˜æ–‡åè®®èŠ‚ç‚¹")

    return []

# ----- è§£æç›¸å…³ -----
def parse_proxies_from_content(content):
    try:
        data = yaml.safe_load(content)
        if isinstance(data, dict):
            proxies = data.get('proxies', [])
            if isinstance(proxies, list):
                return proxies
        elif isinstance(data, list):
            return data
    except Exception:
        pass
    return []

def is_base64(text):
    try:
        s = ''.join(text.split())
        if not s or len(s) % 4 != 0:
            return False
        if not re.match(r'^[A-Za-z0-9+/=]+$', s):
            return False
        base64.b64decode(s, validate=True)
        return True
    except Exception:
        return False

def decode_base64_and_parse(content):
    try:
        decoded = base64.b64decode(''.join(content.split())).decode('utf-8', errors='ignore')
        proxies = []
        success_count = defaultdict(int)
        failure_count = defaultdict(int)
        for line in decoded.splitlines():
            line = line.strip()
            if not line:
                continue
            proxy = None
            proto = None
            if line.startswith('vmess://'):
                proto = 'vmess'
                proxy = parse_vmess_node(line)
            elif line.startswith('vless://'):
                proto = 'vless'
                proxy = parse_vless_node(line)
            elif line.startswith('ssr://'):
                proto = 'ssr'
                proxy = parse_ssr_node(line)
            elif line.startswith('ss://'):
                proto = 'ss'
                proxy = parse_ss_node(line)
            elif line.startswith('trojan://'):
                proto = 'trojan'
                proxy = parse_trojan_node(line)
            elif line.startswith('hysteria://'):
                proto = 'hysteria'
                proxy = parse_hysteria_node(line)
            elif line.startswith('hysteria2://'):
                proto = 'hysteria2'
                proxy = parse_hysteria2_node(line)
            else:
                continue

            if proxy:
                proxies.append(proxy)
                success_count[proto] += 1
            else:
                failure_count[proto] += 1

        for proto, count in success_count.items():
            print(f"  - Base64 è§£ç è§£æå®Œæˆï¼Œ{proto} èŠ‚ç‚¹æˆåŠŸæ•°ï¼š{count}")
        for proto, count in failure_count.items():
            print(f"  - Base64 è§£ç è§£æå¤±è´¥ï¼Œ{proto} èŠ‚ç‚¹å¤±è´¥æ•°ï¼š{count}")

        return proxies
    except Exception as e:
        print(f"  - Base64 è§£ç è§£æå¼‚å¸¸: {e}")
        return []


# ----- åè®®è§£æå®ç° -----
def parse_vmess_node(line):
    try:
        parsed = urlparse(line)
        if parsed.scheme != 'trojan':
            return None
        password = parsed.username or ''
        server = parsed.hostname or ''
        port = parsed.port or 0
        params = parse_qs(parsed.query)
        node = {
            'name': unquote(parsed.fragment) if parsed.fragment else f"trojan_{server}",
            'type': 'trojan',
            'server': server,
            'port': port,
            'password': password,
            'sni': params.get('sni', [''])[0],
            'skip-cert-verify': params.get('allowInsecure', ['false'])[0].lower() == 'true',
            'udp': True,
            'alpn': params.get('alpn', []),
            'tls': True,
        }
        return node
    except Exception:
        return None

def parse_hysteria_node(line):
    try:
        parsed = urlparse(line)
        if parsed.scheme != 'hysteria':
            return None
        params = parse_qs(parsed.query)
        node = {
            'name': unquote(parsed.fragment) or f"hysteria_{parsed.hostname}",
            'type': 'hysteria',
            'server': parsed.hostname,
            'port': int(parsed.port or 0),
            'auth': params.get('auth', [''])[0],
            'protocol': params.get('protocol', ['udp'])[0],
            'insecure': params.get('insecure', ['false'])[0].lower() == 'true',
            'obfs': params.get('obfs', [''])[0],
            'udp': True,
        }
        return node
    except Exception:
        return None

def parse_hysteria2_node(line):
    try:
        parsed = urlparse(line)
        if parsed.scheme != 'hysteria2':
            return None
        params = parse_qs(parsed.query)
        node = {
            'name': unquote(parsed.fragment) or f"hysteria2_{parsed.hostname}",
            'type': 'hysteria2',
            'server': parsed.hostname,
            'port': int(parsed.port or 0),
            'auth': params.get('auth', [''])[0],
            'protocol': params.get('protocol', ['udp'])[0],
            'insecure': params.get('insecure', ['false'])[0].lower() == 'true',
            'obfs': params.get('obfs', [''])[0],
            'udp': True,
        }
        return node
    except Exception:
        return None
# ----- åˆå¹¶å»é‡ -----
def get_proxy_key(proxy):
    try:
        key = f"{proxy.get('server', '')}:{proxy.get('port', 0)}|"
        if 'uuid' in proxy:
            key += proxy['uuid']
        elif 'password' in proxy:
            key += proxy['password']
        else:
            key += proxy.get('name', '')
        return hashlib.md5(key.encode('utf-8')).hexdigest()
    except:
        return None

def merge_and_deduplicate_proxies(proxies):
    unique = {}
    for proxy in proxies:
        if not isinstance(proxy, dict) or 'name' not in proxy:
            continue
        k = get_proxy_key(proxy)
        if k and k not in unique:
            unique[k] = proxy
    return list(unique.values())

# ----- é‡ç‚¹å‡½æ•°ï¼šé‡å‘½åæ’åº -----
def process_and_rename_proxies(proxies):
    country_counters = defaultdict(int)
    final_proxies = []
    all_country_names = set()
    # æ„é€ æ‰€æœ‰å¯èƒ½å›½å®¶åç§°é›†åˆ
    
    for rules in CUSTOM_REGEX_RULES.values():
        # å»ºè®® here åº”è¯¥ç”¨æ›´åˆé€‚çš„æ–¹å¼è·å–å›½å®¶å
        all_country_names.update(rules['pattern'].split('|'))

    all_country_names.update(CHINESE_COUNTRY_MAP.keys())
    all_country_names.update(CHINESE_COUNTRY_MAP.values())
    all_country_names.update(COUNTRY_NAME_TO_CODE_MAP.keys())

    sorted_country_names = sorted(all_country_names, key=len, reverse=True)
    country_pattern = re.compile('|'.join(map(re.escape, sorted_country_names)), re.IGNORECASE)
    speed_pattern = re.compile(r'(\d+(?:\.\d+)?)\s*(M|K)?B/s', re.IGNORECASE)

    def remove_all_flag_emojis(text):
        return FLAG_EMOJI_PATTERN.sub('', text).strip()

    def replace_country_code(text):
        return CHINESE_COUNTRY_MAP.get(text.upper(), text)

    for proxy in proxies:
        original_name = proxy.get('name', '').strip()
        clean_name = remove_all_flag_emojis(original_name)
        clean_name = JUNK_PATTERNS.sub('', clean_name)
        
        speed_text = ''
        speed_match = speed_pattern.search(clean_name)
        if speed_match:
            number, unit = speed_match.groups()
            unit = unit.upper() if unit else ''
            speed_text = f"{number}{unit}B/s"
            clean_name = speed_pattern.sub('', clean_name, count=1).strip()

        country_match = country_pattern.search(clean_name)
        if country_match:
            region_name_raw = country_match.group(0)
            region_name = replace_country_code(region_name_raw)

            # å†æ¬¡å°è¯•æ›¿æ¢è‹±æ–‡ç®€å†™ä¸ºä¸­æ–‡å
            for eng, chn in CHINESE_COUNTRY_MAP.items():
                if re.fullmatch(re.escape(region_name), eng, re.IGNORECASE):
                    region_name = chn
                    break

            country_counters[region_name] += 1
            seq_num = country_counters[region_name]
            region_code = COUNTRY_NAME_TO_CODE_MAP.get(region_name) or CUSTOM_REGEX_RULES.get(region_name, {}).get('code', '')

            flag_emoji = get_country_flag_emoji(region_code)
            new_name = f"{flag_emoji}{region_name}-{seq_num}"
            if speed_text:
                new_name += f"|{speed_text}"
            proxy['name'] = new_name
        else:
            proxy['name'] = original_name
        final_proxies.append(proxy)
    return final_proxies

# ----- æµ‹é€Ÿ -----
def test_single_proxy_socket(proxy):
    server = proxy.get('server')
    port = proxy.get('port')
    if not server or not port:
        return None
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(SOCKET_TIMEOUT)
        start = time.time()
        sock.connect((str(server), int(port)))
        end = time.time()
        proxy['delay'] = int((end - start) * 1000)
        return proxy
    except Exception:
        return None
    finally:
        if 'sock' in locals():
            sock.close()

def speed_test_proxies(proxies):
    print(f"å¼€å§‹æµ‹é€Ÿ: å…± {len(proxies)} ä¸ªèŠ‚ç‚¹")
    fast_proxies = []
    total = len(proxies)
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_TEST_WORKERS) as executor:
        futures = {executor.submit(test_single_proxy_socket, p): p for p in proxies}
        for i, f in enumerate(concurrent.futures.as_completed(futures), 1):
            result = f.result()
            if i % 100 == 0 or i == total:
                print(f"\ræµ‹é€Ÿè¿›åº¦: {i}/{total}", end='', flush=True)
            if result:
                fast_proxies.append(result)
    print()
    print(f"æµ‹é€Ÿå®Œæˆ: æœ‰æ•ˆèŠ‚ç‚¹ {len(fast_proxies)}")
    return fast_proxies

# ----- é…ç½®æ–‡ä»¶ç”Ÿæˆ -----
def generate_config(proxies):
    if not proxies:
        return None
    proxy_names = [p['name'] for p in proxies]
    clean_proxies = [{k: v for k, v in p.items() if k not in ['region', 'delay']} for p in proxies]
    return {
        'mixed-port': 7890,
        'allow-lan': True,
        'bind-address': '*',
        'mode': 'rule',
        'log-level': 'info',
        'external-controller': '127.0.0.1:9090',
        'dns': {
            'enable': True,
            'listen': '0.0.0.0:53',
            'enhanced-mode': 'fake-ip',
            'fake-ip-range': '198.18.0.1/16',
            'nameserver': ['223.5.5.5', '119.29.29.29'],
            'fallback': ['https://dns.google/dns-query', 'https://1.1.1.1/dns-query']
        },
        'proxies': clean_proxies,
        'proxy-groups': [
            {'name': 'ğŸš€ èŠ‚ç‚¹é€‰æ‹©', 'type': 'select',
             'proxies': ['â™»ï¸ è‡ªåŠ¨é€‰æ‹©', 'ğŸ”¯ æ•…éšœè½¬ç§»', 'DIRECT'] + proxy_names},
            {'name': 'â™»ï¸ è‡ªåŠ¨é€‰æ‹©', 'type': 'url-test', 'proxies': proxy_names,
             'url': 'http://www.gstatic.com/generate_204', 'interval': 300},
            {'name': 'ğŸ”¯ æ•…éšœè½¬ç§»', 'type': 'fallback', 'proxies': proxy_names,
             'url': 'http://www.gstatic.com/generate_204', 'interval': 300}],
        'rules': ['GEOIP,CN,DIRECT', 'MATCH,ğŸš€ èŠ‚ç‚¹é€‰æ‹©']
    }

# ------------------ å¤šåŒºå—è¯»å–ä¸å¤„ç† ------------------
def parse_url_txt_to_blocks():
    if not os.path.exists(URL_FILE):
        print(f"æ–‡ä»¶æœªæ‰¾åˆ°: {URL_FILE}")
        return []
    with open(URL_FILE, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    blocks = []
    current_block = {'title': None, 'lines': []}
    for line in lines:
        stripped = line.strip()
        if stripped.startswith('#'):
            if current_block['title']:
                blocks.append(current_block)
            current_block = {'title': stripped, 'lines': []}
        else:
            if stripped:
                current_block['lines'].append(stripped)
    if current_block['title']:
        blocks.append(current_block)
    return blocks

def extract_urls_from_lines(lines):
    url_pattern = re.compile(r'https?://[^\s]+', re.IGNORECASE)
    urls = []
    for line in lines:
        urls.extend(url_pattern.findall(line))
    return urls

def process_block_to_yaml(block):
    title = block['title']
    lines = block['lines']
    urls = extract_urls_from_lines(lines)
    if not urls:
        print(f"{title} åŒºå—æ— æœ‰æ•ˆè®¢é˜…ï¼Œè·³è¿‡ã€‚")
        return
    print(f"\nå¤„ç†åŒºå—ï¼š{title} | {len(urls)} ä¸ªè®¢é˜…é“¾æ¥")
    all_proxies = []
    for url in urls:
        all_proxies.extend(download_subscription(url))
    if not all_proxies:
        print(f"{title} è®¢é˜…ä¸‹è½½å¤±è´¥æˆ–æ— èŠ‚ç‚¹ï¼Œè·³è¿‡ã€‚")
        return
    unique_proxies = merge_and_deduplicate_proxies(all_proxies)
    if ENABLE_SPEED_TEST:
        tested_proxies = speed_test_proxies(unique_proxies)
        if not tested_proxies:
            print(f"{title} æµ‹é€Ÿæ— å¯ç”¨èŠ‚ç‚¹ï¼Œä½¿ç”¨æ‰€æœ‰èŠ‚ç‚¹ã€‚")
            tested_proxies = unique_proxies
    else:
        tested_proxies = unique_proxies
    region_order = {r: i for i, r in enumerate(REGION_PRIORITY)}
    tested_proxies.sort(key=lambda p: (region_order.get(p.get('region', 'æœªçŸ¥'), 999), p.get('delay', 9999)))
    final_proxies = process_and_rename_proxies(tested_proxies)
    config = generate_config(final_proxies)
    if not config:
        print(f"{title} é…ç½®ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡ã€‚")
        return
    filename = sanitize_filename(title) + ".yaml"
    filepath = os.path.join(OUTPUT_DIR, filename)
    with open(filepath, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, allow_unicode=True, sort_keys=False, indent=2)
    print(f"{title} é…ç½®å·²ç”Ÿæˆï¼š{filepath}ï¼ŒèŠ‚ç‚¹æ•°ï¼š{len(final_proxies)}")

def main():
    print("=" * 60)
    print("å›ºå®šé“¾æ¥è·å–èŠ‚ç‚¹è„šæœ¬ V2")
    print(f"æ—¶é—´: {datetime.now(timezone(timedelta(hours=8))).strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    blocks = parse_url_txt_to_blocks()
    if not blocks:
        print("æœªæ£€æµ‹åˆ°æœ‰æ•ˆåŒºå—ï¼Œé€€å‡ºã€‚")
        return
    for block in blocks:
        process_block_to_yaml(block)
    print(f"\nå…¨éƒ¨åŒºå—å¤„ç†å®Œæˆï¼Œé…ç½®æ–‡ä»¶å­˜æ”¾äºï¼š{OUTPUT_DIR}")

if __name__ == "__main__":
    main()
