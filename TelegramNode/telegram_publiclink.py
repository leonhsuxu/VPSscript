# -*- coding: utf-8 -*-
"""
æ–‡ä»¶å: Telegram.Node_Clash-Speedtestæµ‹è¯•ç‰ˆ V1.r1
è„šæœ¬è¯´æ˜:
æœ¬è„šæœ¬å®ç°ä»æŒ‡å®š Telegram é¢‘é“è‡ªåŠ¨çˆ¬å–è®¢é˜…é“¾æ¥ï¼›
ä¸‹è½½å¹¶è§£æå„ç§ä»£ç†è®¢é˜…èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬ vmess, vless, ssr, ss, trojan, hysteriaåŠhysteria2ç­‰åè®®ï¼‰ï¼Œ
æ”¯æŒèŠ‚ç‚¹å»é‡ã€åœ°åŒºè¯†åˆ«ä¸é‡å‘½åï¼Œå¹¶ä½¿ç”¨ Clash æ ¸å¿ƒç¨‹åºè¿›è¡ŒèŠ‚ç‚¹æµ‹é€Ÿï¼ˆå»¶è¿Ÿæµ‹è¯•ï¼‰ï¼›
æœ€ç»ˆç”Ÿæˆå¯ç”¨äº Clash ä½¿ç”¨çš„ YAML é…ç½®æ–‡ä»¶ã€‚
ä¸»è¦åŠŸèƒ½:
1. ä» Telegram æŒ‡å®šé¢‘é“æŠ“å–å¸¦æœ‰è®¢é˜…é“¾æ¥çš„æ¶ˆæ¯ï¼Œæ”¯æŒæ—¶é—´çª—å£è¿‡æ»¤æ–°æ¶ˆæ¯ã€‚
2. æ”¯æŒå¤šç§å¸¸è§ä»£ç†åè®®çš„èŠ‚ç‚¹è§£æï¼Œä»¥åŠè¯†åˆ«èŠ‚ç‚¹æ‰€åœ¨åŒºåŸŸã€‚
3. é‡‡ç”¨å‘½ä»¤è¡Œæ¨¡å¼è°ƒç”¨ clash æ ¸å¿ƒç¨‹åºè¿›è¡ŒèŠ‚ç‚¹å»¶è¿Ÿæµ‹è¯•ï¼Œç­›é€‰æœ‰æ•ˆèŠ‚ç‚¹ã€‚
4. æ ¹æ®èŠ‚ç‚¹åœ°åŒºä¸å»¶è¿Ÿè‡ªåŠ¨æ’åºå’Œå½’ç±»ï¼Œç”Ÿæˆæœ€ç»ˆé…ç½®æ–‡ä»¶ã€‚
5. ç¯å¢ƒå˜é‡é…ç½®çµæ´»ï¼Œæ–¹ä¾¿é›†æˆè‡ªåŠ¨åŒ–æµç¨‹ã€‚
"""
import os
import re
import sys
import json
import time
import yaml
import base64
import hashlib
import shutil
import asyncio
import logging
import tempfile
import subprocess
import threading
from datetime import datetime, timedelta, timezone
from collections import defaultdict
from urllib.parse import urlparse, parse_qs, quote, unquote
import threading
import requests
from zoneinfo import ZoneInfo
from telethon import TelegramClient
from telethon.sessions import StringSession
from concurrent.futures import ThreadPoolExecutor, as_completed
from telethon import TelegramClient
import logging
from urllib.parse import urlparse, unquote

# å·²ç»åœ¨è„šæœ¬å¼€å¤´é…ç½®logger
logger = logging.getLogger("TelegramNodeClashSpeedtest")
print_lock = threading.Lock()

# --- ç¯å¢ƒå˜é‡è¯»å– ---
API_ID = int(os.environ.get('TELEGRAM_API_ID') or 0)
API_HASH = os.environ.get('TELEGRAM_API_HASH')
STRING_SESSION = os.environ.get('TELEGRAM_STRING_SESSION')
TELEGRAM_CHANNEL_IDS_STR = os.environ.get('TELEGRAM_CHANNEL_IDS', '')
TIME_WINDOW_HOURS = 4  # æŠ“å–å¤šé•¿æ—¶é—´çš„æ¶ˆæ¯ï¼Œå•ä½ä¸ºå°æ—¶ã€‚
MIN_EXPIRE_HOURS = 2   # è®¢é˜…åœ°å€å‰©ä½™æ—¶é—´æœ€å°è¿‡æœŸï¼Œå•ä½ä¸ºå°æ—¶ã€‚
OUTPUT_FILE = 'flclashyaml/Tg-node.yaml'  # è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œç”¨äºä¿å­˜ç”Ÿæˆçš„é…ç½®æˆ–ç»“æœã€‚
ENABLE_SPEED_TEST = True  # æ˜¯å¦å¯ç”¨é€Ÿåº¦æµ‹è¯•åŠŸèƒ½ï¼ŒTrueè¡¨ç¤ºå¯ç”¨ã€‚
MAX_TEST_WORKERS = 32    # é€Ÿåº¦æµ‹è¯•æ—¶æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°ï¼Œæ§åˆ¶æµ‹è¯•çš„å¹¶è¡Œåº¦ã€‚
SOCKET_TIMEOUT = 3       # å¥—æ¥å­—è¿æ¥è¶…æ—¶æ—¶é—´ï¼Œå•ä½ä¸ºç§’
HTTP_TIMEOUT = 5         # HTTPè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œå•ä½ä¸ºç§’
HTTP_TEST_URL = 'http://www.gstatic.com/generate_204'
ALLOWED_REGIONS = {
    'é¦™æ¸¯', 'å°æ¹¾', 'æ—¥æœ¬', 'æ–°åŠ å¡', 'éŸ©å›½', 'é©¬æ¥è¥¿äºš', 'æ³°å›½',
    'å°åº¦', 'è²å¾‹å®¾', 'å°åº¦å°¼è¥¿äºš', 'è¶Šå—', 'ç¾å›½', 'åŠ æ‹¿å¤§',
    'æ³•å›½', 'è‹±å›½', 'å¾·å›½', 'ä¿„ç½—æ–¯', 'æ„å¤§åˆ©', 'å·´è¥¿',
    'é˜¿æ ¹å»·', 'åœŸè€³å…¶', 'æ¾³å¤§åˆ©äºš'
}
REGION_PRIORITY = [
    'é¦™æ¸¯', 'å°æ¹¾', 'æ—¥æœ¬', 'æ–°åŠ å¡', 'éŸ©å›½', 'é©¬æ¥è¥¿äºš', 'æ³°å›½',
    'å°åº¦', 'è²å¾‹å®¾', 'å°åº¦å°¼è¥¿äºš', 'è¶Šå—', 'ç¾å›½', 'åŠ æ‹¿å¤§',
    'æ³•å›½', 'è‹±å›½', 'å¾·å›½', 'ä¿„ç½—æ–¯', 'æ„å¤§åˆ©', 'å·´è¥¿',
    'é˜¿æ ¹å»·', 'åœŸè€³å…¶', 'æ¾³å¤§åˆ©äºš'
]
CUSTOM_REGEX_RULES = {
    'é¦™æ¸¯': {'code': 'HK', 'pattern': r'é¦™æ¸¯|æ¸¯|HK|Hong\s*Kong'},
    'å°æ¹¾': {'code': 'TW', 'pattern': r'å°æ¹¾|å°|TW|Taiwan'},
    'æ—¥æœ¬': {'code': 'JP', 'pattern': r'æ—¥æœ¬|æ—¥|JP|Japan'},
    'æ–°åŠ å¡': {'code': 'SG', 'pattern': r'æ–°åŠ å¡|SG|Singapore'},
    'éŸ©å›½': {'code': 'KR', 'pattern': r'éŸ©å›½|å—æœé²œ|KR|Korea'},
    'é©¬æ¥è¥¿äºš': {'code': 'MY', 'pattern': r'é©¬æ¥è¥¿äºš|MY|Malaysia'},
    'æ³°å›½': {'code': 'TH', 'pattern': r'æ³°å›½|TH|Thailand'},
    'å°åº¦': {'code': 'IN', 'pattern': r'å°åº¦|IN|India'},
    'è²å¾‹å®¾': {'code': 'PH', 'pattern': r'è²å¾‹å®¾|PH|Philippines'},
    'å°åº¦å°¼è¥¿äºš': {'code': 'ID', 'pattern': r'å°åº¦å°¼è¥¿äºš|å°å°¼|ID|Indonesia'},
    'è¶Šå—': {'code': 'VN', 'pattern': r'è¶Šå—|VN|Vietnam'},
    'ç¾å›½': {'code': 'US', 'pattern': r'ç¾å›½|US|USA|United States'},
    'åŠ æ‹¿å¤§': {'code': 'CA', 'pattern': r'åŠ æ‹¿å¤§|CA|Canada'},
    'æ³•å›½': {'code': 'FR', 'pattern': r'æ³•å›½|FR|France'},
    'è‹±å›½': {'code': 'GB', 'pattern': r'è‹±å›½|GB|UK|United Kingdom'},
    'å¾·å›½': {'code': 'DE', 'pattern': r'å¾·å›½|DE|Germany'},
    'ä¿„ç½—æ–¯': {'code': 'RU', 'pattern': r'ä¿„ç½—æ–¯|RU|Russia'},
    'æ„å¤§åˆ©': {'code': 'IT', 'pattern': r'æ„å¤§åˆ©|IT|Italy'},
    'å·´è¥¿': {'code': 'BR', 'pattern': r'å·´è¥¿|BR|Brazil'},
    'é˜¿æ ¹å»·': {'code': 'AR', 'pattern': r'é˜¿æ ¹å»·|AR|Argentina'},
    'åœŸè€³å…¶': {'code': 'TR', 'pattern': r'åœŸè€³å…¶|TR|Turkey'},
    'æ¾³å¤§åˆ©äºš': {'code': 'AU', 'pattern': r'æ¾³å¤§åˆ©äºš|AU|Australia'},
}
FLAG_EMOJI_PATTERN = re.compile(r'[\U0001F1E6-\U0001F1FF]{2}')
BJ_TZ = ZoneInfo("Asia/Shanghai")

def get_country_flag_emoji(code):
    if not code or len(code) != 2:
        return "â“"
    return "".join(chr(0x1F1E6 + ord(c.upper()) - ord('A')) for c in code)

def preprocess_regex_rules():
    for region in CUSTOM_REGEX_RULES:
        CUSTOM_REGEX_RULES[region]['pattern'] = '|'.join(
            sorted(CUSTOM_REGEX_RULES[region]['pattern'].split('|'), key=len, reverse=True)
        )

def load_existing_proxies_and_state():
    existing_proxies = []
    last_message_ids = {}
    if os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                loaded_yaml = yaml.safe_load(f)
                if isinstance(loaded_yaml, dict):
                    existing_proxies = loaded_yaml.get('proxies', [])
                    if not isinstance(existing_proxies, list):
                        existing_proxies = []
                    last_message_ids = loaded_yaml.get('last_message_ids', {})
                    if not isinstance(last_message_ids, dict):
                        last_message_ids = {}
                elif isinstance(loaded_yaml, list):
                    existing_proxies = [p for p in loaded_yaml if isinstance(p, dict)]
        except Exception as e:
            print(f"è¯»å– {OUTPUT_FILE} å¤±è´¥: {e}")
    return existing_proxies, last_message_ids

def extract_valid_subscribe_links(text):
    MIN_HOURS_LEFT = MIN_EXPIRE_HOURS
    link_pattern = re.compile(
        r'(?:è®¢é˜…é“¾æ¥|è®¢é˜…åœ°å€|è®¢é˜…)[\s:ï¼š`]*?(https?://[A-Za-z0-9\-._~:/?#[\]@!$&\'()*+,;=%]+)'
    )
    expire_patterns = [
        r'åˆ°æœŸæ—¶é—´[:ï¼š]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2}\s+\d{2}:\d{2}:\d{2})',
        r'è¿‡æœŸæ—¶é—´[:ï¼š]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2}\s+\d{2}:\d{2}:\d{2})',
        r'è¯¥è®¢é˜…å°†äº(\d{4}[-/]\d{1,2}[-/]\d{1,2}\s+\d{2}:\d{2}:\d{2})(?:\s*\+\d{4}\s*[A-Za-z]{3})?è¿‡æœŸ',
        r'è¿‡æœŸ[:ï¼š]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
        r'åˆ°æœŸ[:ï¼š]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
        r'è¯¥è®¢é˜…å°†äºæœªçŸ¥è¿‡æœŸ',
        r'è¿‡æœŸæ—¶é—´[:ï¼š]\s*é•¿æœŸæœ‰æ•ˆ',
        r'è¿‡æœŸ[:ï¼š]\s*æœªçŸ¥/æ— é™',
    ]
    text_single_line = text.replace('\n', ' ')
    expire_time = None
    for patt in expire_patterns:
        match = re.search(patt, text_single_line)
        if match:
            if 'æœªçŸ¥' in match.group(0) or 'é•¿æœŸæœ‰æ•ˆ' in match.group(0) or 'æ— é™' in match.group(0):
                expire_time = None
                break
            if match.lastindex:
                dt_str = match.group(1)
                fmt_candidates = ['%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S', '%Y-%m-%d', '%Y/%m/%d']
                for fmt in fmt_candidates:
                    try:
                        dt = datetime.strptime(dt_str, fmt)
                        if fmt in ('%Y-%m-%d', '%Y/%m/%d'):
                            dt = dt.replace(hour=23, minute=59, second=59)
                        expire_time = dt.replace(tzinfo=BJ_TZ)
                        break
                    except Exception:
                        continue
            break
    now = datetime.now(BJ_TZ)
    valid_links = []
    links = link_pattern.findall(text)
    for url in links:
        if expire_time is not None:
            hours_left = (expire_time - now).total_seconds() / 3600
            if hours_left < MIN_HOURS_LEFT:
                continue
        valid_links.append(url)
    return valid_links

# ==========================
# æ›¿æ¢äº† scrape_telegram_links ä¸º B ç‰ˆæœ¬æ›´å®Œå–„çš„å®ç°
async def scrape_telegram_links(last_message_ids=None):
    if last_message_ids is None:
        last_message_ids = {}
    if not all([API_ID, API_HASH, STRING_SESSION, TELEGRAM_CHANNEL_IDS_STR]):
        print("âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡ (API_ID, API_HASH, STRING_SESSION, TELEGRAM_CHANNEL_IDS)ã€‚")
        return [], last_message_ids
    TARGET_CHANNELS = [line.strip() for line in TELEGRAM_CHANNEL_IDS_STR.split('\n')
                       if line.strip() and not line.strip().startswith('#')]
    if not TARGET_CHANNELS:
        print("âŒ é”™è¯¯: TELEGRAM_CHANNEL_IDS ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆé¢‘é“ IDã€‚")
        return [], last_message_ids
    print(f"â–¶ï¸ é…ç½®æŠ“å– {len(TARGET_CHANNELS)} ä¸ªé¢‘é“: {TARGET_CHANNELS}")
    try:
        client = TelegramClient(StringSession(STRING_SESSION), API_ID, API_HASH)
        await client.connect()
        me = await client.get_me()
        print(f"âœ… ä»¥ {me.first_name} (@{me.username}) çš„èº«ä»½æˆåŠŸè¿æ¥")
    except Exception as e:
        print(f"âŒ é”™è¯¯: è¿æ¥ Telegram æ—¶å‡ºé”™: {e}")
        return [], last_message_ids
    bj_now = datetime.now(BJ_TZ)
    target_time = (bj_now - timedelta(hours=TIME_WINDOW_HOURS)).astimezone(timezone.utc)
    all_links = set()
    for channel_id in TARGET_CHANNELS:
        print(f"\n ğŸ¯æ­£åœ¨å¤„ç†é¢‘é“: {channel_id} ...")
        try:
            entity = await client.get_entity(channel_id)
        except Exception as e:
            print(f"âŒ é”™è¯¯: æ— æ³•è·å–é¢‘é“å®ä½“ {channel_id}: {e}")
            continue
        last_id = last_message_ids.get(channel_id, 0)
        max_id_found = last_id
        try:
            async for message in client.iter_messages(entity, min_id=last_id + 1, reverse=False):
                if message.date < target_time:
                    break
                if message.text:
                    links = extract_valid_subscribe_links(message.text)
                    for link in links:
                        if link not in all_links:
                            all_links.add(link)
                            print(f"  âœ… æ‰¾åˆ°é“¾æ¥: {link[:70]}...")
                if message.id > max_id_found:
                    max_id_found = message.id
            last_message_ids[channel_id] = max_id_found
        except Exception as e:
            print(f"âŒ é”™è¯¯: ä»é¢‘é“ '{channel_id}' è·å–æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
    await client.disconnect()
    print(f"\nâœ… æŠ“å–å®Œæˆ, å…±æ‰¾åˆ° {len(all_links)} ä¸ªä¸é‡å¤çš„æœ‰æ•ˆé“¾æ¥ã€‚")
    return list(all_links), last_message_ids

# --- B ç‰ˆæœ¬çš„ä¸‹è½½åŠè§£æç›¸å…³å‡½æ•°åˆå…¥ ---

def attempt_download_using_wget(url):
    print(f"  â¬‡ï¸ æ­£åœ¨ä½¿ç”¨ wget ä¸‹è½½: {url[:80]}...")
    if not shutil.which("wget"):
        print("  âœ— é”™è¯¯: wget æœªå®‰è£…ï¼Œæ— æ³•æ‰§è¡Œä¸‹è½½ã€‚")
        return None
    try:
        content = subprocess.run(
            ["wget", "-O", "-", "--timeout=30", "--header=User-Agent: Clash", url],
            capture_output=True, text=True, check=True, encoding='utf-8', errors='ignore'
        ).stdout
        return content if content else None
    except subprocess.CalledProcessError as e:
        print(f"  âœ— wget ä¸‹è½½å¤±è´¥: {e.stderr}")
        return None

def attempt_download_using_requests(url):
    print(f"  â¬‡ï¸ æ­£åœ¨ä½¿ç”¨ requests ä¸‹è½½: {url[:80]}...")
    try:
        headers = {'User-Agent': 'Clash'}
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        response.encoding = response.apparent_encoding or 'utf-8'
        return response.text
    except requests.RequestException as e:
        print(f"  âœ— requests ä¸‹è½½å¤±è´¥: {e}")
        return None

def parse_proxies_from_content(content):
    try:
        data = yaml.safe_load(content)
        if isinstance(data, dict):
            proxies = data.get('proxies', [])
            if isinstance(proxies, list):
                return proxies
        elif isinstance(data, list):
            return data
    except Exception:
        pass
    return []

def is_base64(text):
    """
    åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦æ˜¯Base64æ ¼å¼ï¼ˆæ”¯æŒurlsafe base64ï¼‰
    - å…è®¸æ— padding
    - å…è®¸urlsafeå­—ç¬¦é›†ï¼ˆ- å’Œ _ï¼‰
    """
    try:
        s = ''.join(text.strip().split())
        if not s:
            return False
        # base64å­—ç¬¦é›†ï¼ŒåŒ…æ‹¬urlsafeçš„ '-' å’Œ '_'
        if not re.match(r'^[A-Za-z0-9\-_+=]+$', s):
            return False
        # å°è¯•è§£ç ï¼Œè¡¥è¶³padding
        padding_len = (4 - len(s) % 4) % 4
        s_padded = s + ('=' * padding_len)
        base64.urlsafe_b64decode(s_padded)
        return True
    except Exception:
        return False

def parse_vmess_node(line):
    try:
        content_b64 = line[8:]
        decoded = base64.b64decode(content_b64 + '=' * (-len(content_b64) % 4)).decode('utf-8', errors='ignore')
        info = json.loads(decoded)
        node = {
            'name': info.get('ps', 'vmess_node'),
            'type': 'vmess',
            'server': info.get('add') or info.get('host'),
            'port': int(info.get('port', 0)),
            'uuid': info.get('id') or info.get('uuid'),
            'alterId': int(info.get('aid', info.get('alterId', 0))) if str(info.get('aid', '')).isdigit() else 0,
            'cipher': info.get('scy', 'auto'),
            'network': info.get('net', 'tcp'),
            'tls': True if info.get('tls', '').lower() == 'tls' else False,
            'skip-cert-verify': info.get('allowInsecure', False),
            'ws-opts': {},
        }
        if node['network'] == 'ws':
            ws_opts = {
                'path': info.get('path', ''),
                'headers': {'Host': info.get('host', '')} if info.get('host') else {},
            }
            node['ws-opts'] = ws_opts
        return node
    except Exception:
        return None

def parse_vless_node(line):
    try:
        parsed = urlparse(line.strip())
        if parsed.scheme != 'vless':
            return None
        params = parse_qs(parsed.query)
        node = {
            'name': unquote(parsed.fragment) if parsed.fragment else f"vless_{parsed.hostname}",
            'type': 'vless',
            'server': parsed.hostname,
            'port': int(parsed.port or 0),
            'uuid': parsed.username,
            'encryption': 'none',
            'flow': params.get('flow', [''])[0],
            'tls': (parsed.query.lower().find('tls') != -1) or ('tls' in params),
            'skip-cert-verify': params.get('allowInsecure', ['false'])[0].lower() == 'true',
            'network': params.get('type', ['tcp'])[0],
            'host': params.get('host', [''])[0],
            'path': params.get('path', [''])[0],
            'sni': params.get('sni', [''])[0],
        }
        if node['network'] == 'ws':
            node['ws-opts'] = {'path': node['path'], 'headers': {'Host': node['host']} if node['host'] else {}}
        return node
    except Exception:
        return None

def parse_ssr_node(line):
    try:
        ssr_b64 = line[6:]
        ssr_decoded = base64.urlsafe_b64decode(ssr_b64 + '=' * (-len(ssr_b64) % 4)).decode('utf-8', errors='ignore')
        parts = ssr_decoded.split('/?')
        main = parts[0]
        params_str = parts[1] if len(parts) > 1 else ''
        server, port, protocol, method, obfs, password_b64 = main.split(':', 5)
        password = base64.urlsafe_b64decode(password_b64 + '=' * (-len(password_b64) % 4)).decode('utf-8', errors='ignore')
        params = {}
        for param in params_str.split('&'):
            if '=' in param:
                k, v = param.split('=', 1)
                params[k] = v
        remark = unquote(params.get('remarks', ''))
        node = {
            'name': remark or f"ssr_{server}",
            'type': 'ssr',
            'server': server,
            'port': int(port),
            'cipher': method,
            'protocol': protocol,
            'obfs': obfs,
            'password': password,
            'udp': params.get('udp', 'false').lower() == 'true'
        }
        return node
    except Exception:
        return None

def parse_ss_node(line):
    """
    è§£æSSåè®®èŠ‚ç‚¹å­—ç¬¦ä¸²ï¼Œæ”¯æŒä»¥ä¸‹æ ¼å¼ï¼š
    - æ˜æ–‡æ ¼å¼ï¼šss://method:password@server:port#remark
    - Base64ç¼–ç æ ¼å¼ï¼šss://base64(method:password@server:port)#remark
    è¿”å›è§£æå­—å…¸æˆ–Noneã€‚
    """
    try:
        line = line.strip()
        if not line.startswith('ss://'):
            return None
        content = line[5:]

        # æ‹†åˆ†å¤‡æ³¨éƒ¨åˆ†
        main_part, sep, remark_part = content.partition('#')
        remark = unquote(remark_part) if sep else ''

        if '@' in main_part:
            # æ˜æ–‡æ ¼å¼ï¼Œç›´æ¥ç”¨urlparseè§£æ
            parsed = urlparse('ss://' + main_part)
            user_pass = parsed.netloc.split('@')[0]
            if ':' not in user_pass:
                logger.debug(f"è§£æå¤±è´¥ï¼Œuser_passæ ¼å¼é”™è¯¯: {user_pass}")
                return None
            method, password = user_pass.split(':', 1)

            server = parsed.hostname
            port = parsed.port
            if not (server and port):
                logger.debug(f"è§£æå¤±è´¥ï¼Œserveræˆ–portç¼ºå¤±: server={server}, port={port}")
                return None

            return {
                'name': remark or f"ss_{server}:{port}",
                'type': 'ss',
                'server': server,
                'port': port,
                'cipher': method,
                'password': password,
                'udp': True
            }
        else:
            # base64ç¼–ç æ ¼å¼
            ss_b64 = main_part
            # åˆ¤æ–­æ˜¯å¦ä¸ºåˆæ³•Base64å­—ç¬¦ä¸²
            from base64 import urlsafe_b64decode
            import re

            def is_base64(s):
                s = s.strip()
                # base64å­—ç¬¦é›†ï¼ŒåŒ…æ‹¬urlsafeçš„ '-' å’Œ '_'
                if not re.match(r'^[A-Za-z0-9\-_+=]+$', s):
                    return False
                try:
                    padding_len = (4 - len(s) % 4) % 4
                    s_padded = s + ('=' * padding_len)
                    urlsafe_b64decode(s_padded)
                    return True
                except Exception:
                    return False

            if not is_base64(ss_b64):
                logger.debug(f"ä¸æ˜¯åˆæ³•çš„base64ç¼–ç å­—ç¬¦ä¸²: {ss_b64}")
                return None

            padding_len = (4 - len(ss_b64) % 4) % 4
            ss_b64_padded = ss_b64 + ('=' * padding_len)
            decoded = urlsafe_b64decode(ss_b64_padded).decode('utf-8', errors='ignore')

            if '@' not in decoded:
                logger.debug(f"base64è§£ç åç¼ºå°‘@ç¬¦å·: {decoded}")
                return None

            method_password, server_port = decoded.split('@', 1)
            if ':' not in method_password:
                logger.debug(f"æ ¼å¼é”™è¯¯ï¼Œmethod_passwordæ— å†’å·: {method_password}")
                return None
            method, password = method_password.split(':', 1)

            if ':' not in server_port:
                logger.debug(f"æ ¼å¼é”™è¯¯ï¼Œserver_portæ— å†’å·: {server_port}")
                return None
            # ç«¯å£åœ¨æœ€åä¸€ä¸ªå†’å·ä¹‹å
            server, port_str = server_port.rsplit(':', 1)
            try:
                port = int(port_str)
            except ValueError:
                logger.debug(f"ç«¯å£è½¬æ¢å¤±è´¥: {port_str}")
                return None

            return {
                'name': remark or f"ss_{server}:{port}",
                'type': 'ss',
                'server': server,
                'port': port,
                'cipher': method,
                'password': password,
                'udp': True
            }
    except Exception as e:
        logger.error(f"è§£æssèŠ‚ç‚¹å¼‚å¸¸: {line}, é”™è¯¯: {e}", exc_info=True)
        return None

def parse_trojan_node(line):
    try:
        parsed = urlparse(line)
        if parsed.scheme != 'trojan':
            return None
        password = parsed.username or ''
        server = parsed.hostname or ''
        port = parsed.port or 0
        params = parse_qs(parsed.query)
        node = {
            'name': unquote(parsed.fragment) if parsed.fragment else f"trojan_{server}",
            'type': 'trojan',
            'server': server,
            'port': port,
            'password': password,
            'sni': params.get('sni', [''])[0],
            'skip-cert-verify': params.get('allowInsecure', ['false'])[0].lower() == 'true',
            'udp': True,
            'alpn': params.get('alpn', []),
            'tls': True,
        }
        return node
    except Exception:
        return None

def parse_hysteria_node(line):
    try:
        parsed = urlparse(line)
        if parsed.scheme != 'hysteria':
            return None
        params = parse_qs(parsed.query)
        node = {
            'name': unquote(parsed.fragment) or f"hysteria_{parsed.hostname}",
            'type': 'hysteria',
            'server': parsed.hostname,
            'port': int(parsed.port or 0),
            'auth': params.get('auth', [''])[0],
            'protocol': params.get('protocol', ['udp'])[0],
            'insecure': params.get('insecure', ['false'])[0].lower() == 'true',
            'obfs': params.get('obfs', [''])[0],
            'udp': True,
        }
        return node
    except Exception:
        return None

def parse_hysteria2_node(line):
    try:
        parsed = urlparse(line)
        if parsed.scheme != 'hysteria2':
            return None
        params = parse_qs(parsed.query)
        auth = parsed.username or ''
        obfs_password = params.get('obfs-password', [''])[0]
        insecure_val = params.get('insecure', ['false'])[0].lower()
        insecure = insecure_val in ('1', 'true', 'yes')
        node = {
            'name': unquote(parsed.fragment) if parsed.fragment else f"hysteria2_{parsed.hostname}",
            'type': 'hysteria2',
            'server': parsed.hostname,
            'port': int(parsed.port or 0),
            'auth': auth,
            'protocol': params.get('protocol', ['udp'])[0],
            'insecure': insecure,
            'obfs': params.get('obfs', [''])[0],
            'obfs-password': obfs_password,
            'udp': params.get('udp', ['true'])[0].lower() == 'true',
        }
        return node
    except Exception:
        return None

def parse_plain_nodes_from_text(text):
    proxies = []
    success_count = defaultdict(int)
    failure_count = defaultdict(int)
    for line in text.splitlines():
        line = line.strip()
        if not line:
            continue
        proxy = None
        proto = None
        if line.startswith('vmess://'):
            proto = 'vmess'
            proxy = parse_vmess_node(line)
        elif line.startswith('vless://'):
            proto = 'vless'
            proxy = parse_vless_node(line)
        elif line.startswith('ssr://'):
            proto = 'ssr'
            proxy = parse_ssr_node(line)
        elif line.startswith('ss://'):
            proto = 'ss'
            proxy = parse_ss_node(line)
        elif line.startswith('trojan://'):
            proto = 'trojan'
            proxy = parse_trojan_node(line)
        elif line.startswith('hysteria://'):
            proto = 'hysteria'
            proxy = parse_hysteria_node(line)
        elif line.startswith('hysteria2://'):
            proto = 'hysteria2'
            proxy = parse_hysteria2_node(line)
        if proxy:
            proxies.append(proxy)
            success_count[proto] += 1
        else:
            failure_count[proto] += 1
    for proto, count in success_count.items():
        print(f"  - æ˜æ–‡åè®®è§£æå®Œæˆï¼Œ{proto} èŠ‚ç‚¹æˆåŠŸæ•°ï¼š{count}")
    for proto, count in failure_count.items():
        print(f"  - æ˜æ–‡åè®®è§£æå¤±è´¥ï¼Œ{proto} èŠ‚ç‚¹å¤±è´¥æ•°ï¼š{count}")
    return proxies

def decode_base64_and_parse(content):
    try:
        decoded = base64.b64decode(''.join(content.split())).decode('utf-8', errors='ignore')
        proxies = []
        success_count = defaultdict(int)
        failure_count = defaultdict(int)
        for line in decoded.splitlines():
            line = line.strip()
            if not line:
                continue
            proxy = None
            proto = None
            if line.startswith('vmess://'):
                proto = 'vmess'
                proxy = parse_vmess_node(line)
            elif line.startswith('vless://'):
                proto = 'vless'
                proxy = parse_vless_node(line)
            elif line.startswith('ssr://'):
                proto = 'ssr'
                proxy = parse_ssr_node(line)
            elif line.startswith('ss://'):
                proto = 'ss'
                proxy = parse_ss_node(line)
            elif line.startswith('trojan://'):
                proto = 'trojan'
                proxy = parse_trojan_node(line)
            elif line.startswith('hysteria://'):
                proto = 'hysteria'
                proxy = parse_hysteria_node(line)
            elif line.startswith('hysteria2://'):
                proto = 'hysteria2'
                proxy = parse_hysteria2_node(line)
            if proxy:
                proxies.append(proxy)
                success_count[proto] += 1
            else:
                failure_count[proto] += 1
        for proto, count in success_count.items():
            print(f"  - Base64 è§£ç è§£æå®Œæˆï¼Œ{proto} èŠ‚ç‚¹æˆåŠŸæ•°ï¼š{count}")
        for proto, count in failure_count.items():
            print(f"  - Base64 è§£ç è§£æå¤±è´¥ï¼Œ{proto} èŠ‚ç‚¹å¤±è´¥æ•°ï¼š{count}")
        return proxies
    except Exception as e:
        print(f"  - Base64 è§£ç è§£æå¼‚å¸¸: {e}")
        return []

def download_and_parse(url):
    # ç”¨ B ç‰ˆæœ¬çš„ä¸‹è½½è§£æé€»è¾‘æ›¿ä»£ A é‡ŒåŸ download_and_parse ä½“
    content = attempt_download_using_wget(url)
    if content is None:
        content = attempt_download_using_requests(url)
    if content is None:
        print(f"  âŒ ä¸‹è½½å¤±è´¥: {url}")
        return []
    proxies = parse_proxies_from_content(content)
    if proxies:
        print(f"  - ç›´æ¥ YAML è§£æè·å– {len(proxies)} ä¸ªèŠ‚ç‚¹")
        return proxies
    proxies = parse_plain_nodes_from_text(content)
    if proxies:
        print(f"  - æ˜æ–‡å†…å®¹è§£æè·å– {len(proxies)} ä¸ªèŠ‚ç‚¹")
        return proxies
    if is_base64(content):
        print(f"  - å†…å®¹ä¸º Base64 ç¼–ç ï¼Œæ­£åœ¨è§£ç è§£æ...")
        proxies = decode_base64_and_parse(content)
        if proxies:
            return proxies
        else:
            print(f"  - Base64 è§£ç æ— æœ‰æ•ˆèŠ‚ç‚¹")
            return []
    print(f"  - å†…å®¹ä¸ç¬¦åˆå·²çŸ¥æ ¼å¼ï¼Œæœªæ‰¾åˆ°æœ‰æ•ˆèŠ‚ç‚¹")
    return []

# --- ä¸‹é¢ä¿æŒåŸAç‰ˆæµ‹é€Ÿã€å»é‡ã€æ’åºç­‰é€»è¾‘ ---


def get_proxy_key(proxy):
    unique_part = proxy.get('uuid') or proxy.get('password') or ''
    return hashlib.md5(
        f"{proxy.get('server','')}:{proxy.get('port',0)}|{unique_part}".encode()
    ).hexdigest()

def is_valid_ss_cipher(cipher):
    """
    åˆ¤æ–­ssèŠ‚ç‚¹cipherå­—æ®µæ˜¯å¦åˆæ³•ï¼Œé¿å…è¢«é”™è¯¯çš„Base64æˆ–å…¶å®ƒå­—ç¬¦ä¸²æ±¡æŸ“ã€‚
    è¿™é‡Œåˆ—ä¸¾äº†Clashå¸¸è§æ”¯æŒçš„ssåŠ å¯†æ–¹æ³•ï¼Œå¿…è¦æ—¶ä½ å¯æ ¹æ®å®é™…å¢åŠ æˆ–ä¿®æ”¹ã€‚

    å‚æ•°:
        cipher (str): ssèŠ‚ç‚¹ä¸­cipherå­—æ®µ

    è¿”å›:
        bool: æ˜¯å¦æœ‰æ•ˆ
    """
    if not cipher:
        return False
    valid_ciphers = {
        'aes-256-gcm', 'aes-128-gcm', 'chacha20-ietf-poly1305',
        'aes-256-cfb', 'aes-128-cfb', 'chacha20-ietf', 'xchacha20',
        'aes-128-ctr', 'aes-256-ctr', 'rc4-md5'
    }
    return cipher.lower() in valid_ciphers


def is_valid_proxy(proxy):
    """
    éªŒè¯ä»£ç†èŠ‚ç‚¹åŸºæœ¬å®Œæ•´æ€§å’Œå­—æ®µæœ‰æ•ˆæ€§ï¼Œå¢åŠ å¯¹ssåŠ å¯†æ–¹æ³•çš„å•ç‹¬éªŒè¯ã€‚

    å‚æ•°:
        proxy (dict): ä»£ç†èŠ‚ç‚¹å­—å…¸

    è¿”å›:
        bool: åˆæ³•ä¸ºTrueï¼Œå¦åˆ™False
    """
    if not isinstance(proxy, dict):
        return False
    required_keys = ['name', 'server', 'port', 'type']
    if not all(key in proxy for key in required_keys):
        return False

    allowed_types = {'http', 'socks5', 'trojan', 'vless', 'ss', 'vmess', 'ssr', 'hysteria', 'hysteria2'}
    if proxy['type'] not in allowed_types:
        return False

    port = proxy.get('port')
    if not isinstance(port, int) or not (1 <= port <= 65535):
        return False

    # SSåè®®ç‰¹æœ‰çš„åŠ å¯†æ–¹æ³•å­—æ®µæ£€æŸ¥ï¼Œç¡®ä¿ä¸ºæœ‰æ•ˆcipher
    if proxy['type'] == 'ss':
        cipher = proxy.get('cipher', '')
        if not is_valid_ss_cipher(cipher):
            if cipher:
                print(f"âš ï¸ æ— æ•ˆçš„ ss cipher: {cipher}ï¼ŒèŠ‚ç‚¹å: {proxy.get('name')}")
            return False

    return True

def identify_regions_only(proxies):
    identified = []
    for p in proxies:
        matched_region = None
        for region_name, info in CUSTOM_REGEX_RULES.items():
            if re.search(info['pattern'], p.get('name', ''), re.IGNORECASE):
                matched_region = {'name': region_name, 'code': info['code']}
                break
        if matched_region:
            p['region_info'] = matched_region
            identified.append(p)
    return identified

def process_proxies(proxies):
    identified = []
    for p in proxies:
        matched_region = None
        for region_name, info in CUSTOM_REGEX_RULES.items():
            if re.search(info['pattern'], p.get('name', ''), re.IGNORECASE):
                matched_region = {'name': region_name, 'code': info['code']}
                break
        if matched_region is None:
            continue
        if matched_region['name'] not in ALLOWED_REGIONS:
            continue
        p['region_info'] = matched_region
        identified.append(p)
    counters = defaultdict(lambda: defaultdict(int))
    master_pattern = re.compile(
        '|'.join(sorted([p for r in CUSTOM_REGEX_RULES.values() for p in r['pattern'].split('|')], key=len, reverse=True)),
        re.IGNORECASE
    )
    final = []
    for p in identified:
        info = p['region_info']
        match = FLAG_EMOJI_PATTERN.search(p['name'])
        flag = match.group(0) if match else get_country_flag_emoji(info['code'])
        clean_name = master_pattern.sub('', FLAG_EMOJI_PATTERN.sub('', p['name'], 1)).strip()
        clean_name = re.sub(r'^\W+|\W+$', '', clean_name)
        feature = re.sub(r'\s+', ' ', clean_name).strip()
        if not feature:
            count = sum(1 for fp in final if fp['region_info']['name'] == info['name']) + 1
            feature = f"{info['code']}{count:02d}"
        base_name = f"{flag} {info['name']} {feature}".strip()
        counters[info['name']][base_name] += 1
        count_ = counters[info['name']][base_name]
        if count_ > 1:
            new_name = f"{base_name} {count_}"
        else:
            new_name = base_name
        p['name'] = new_name
        final.append(p)
    return final


def generate_config(proxies, last_message_ids):
    return {
        'proxies': proxies,
        'last_message_ids': last_message_ids,
    }

def clash_test_proxy_single(proxy: dict, clash_path: str = "clash_core/clash", debug: bool = False) -> int | None:
    """
    ä½¿ç”¨ Clash æ ¸å¿ƒå¯¹å•ä¸ªèŠ‚ç‚¹è¿›è¡Œå»¶è¿Ÿæµ‹è¯•ï¼ˆè¿”å›æ¯«ç§’æˆ– Noneï¼‰
    """
    temp_dir = None
    try:
        # 1. åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp(prefix="clash_test_")

        # 2. ç”Ÿæˆæœ€å°åŒ– config.yaml
        config = {
            "mixed-port": 7890,
            "mode": "direct",
            "log-level": "silent",
            "ipv6": False,
            "proxies": [proxy],
            "proxy-groups": [{"name": "auto", "type": "select", "proxies": [proxy["name"]]}],
            "rules": []
        }
        config_path = os.path.join(temp_dir, "config.yaml")
        with open(config_path, "w", encoding="utf-8") as f:
            yaml.dump(config, f, allow_unicode=True)

        # 3. å¯åŠ¨ Clash å¹¶æµ‹è¯•å»¶è¿Ÿ
        cmd = [
            clash_path,
            "-d", temp_dir,
            "-f", config_path
        ]
        if debug:
            print(f"è°ƒè¯•: å¯åŠ¨ Clash æµ‹è¯•èŠ‚ç‚¹ â†’ {proxy.get('name', 'Unknown')}")

        proc = subprocess.Popen(
            cmd,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL
        )

        # ç­‰å¾… Clash å¯åŠ¨ï¼ˆæœ€å¤š 5 ç§’ï¼‰
        time.sleep(4)

        # å‘èµ·æµ‹é€Ÿè¯·æ±‚
        test_url = f"http://127.0.0.1:7890/proxies/{quote(proxy['name'])}/delay?timeout=5000&url={quote(HTTP_TEST_URL)}"
        start_time = time.time()
        try:
            resp = requests.get(test_url, timeout=6)
            if resp.status_code == 200:
                data = resp.json()
                delay = data.get("delay")
                if isinstance(delay, int) and delay > 0:
                    return delay
        except:
            pass

        # è¶…æ—¶æˆ–å¤±è´¥å¤„ç†
        try:
            proc.wait(timeout=3)
        except:
            proc.kill()

        return None

    except Exception as e:
        if debug:
            print(f"æµ‹é€Ÿå¼‚å¸¸: {proxy.get('name')} â†’ {e}")
        try:
            if 'proc' in locals():
                proc.kill()
        except:
            pass
        return None

    finally:
        # ç¡®ä¿æ¸…ç†ä¸´æ—¶ç›®å½•
        if temp_dir and os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir, ignore_errors=True)
            except:
                pass


# ========== å¤šçº¿ç¨‹æ‰¹é‡æµ‹é€Ÿ + ç²¾ç¡®ç»Ÿè®¡ï¼ˆç›´æ¥æ›¿æ¢ä½ åŸæ¥çš„ batch_test_proxies_clashï¼‰==========
def batch_test_proxies_clash(clash_path: str, proxies: list, max_workers: int = 128):
    success_nodes = []
    stats = {
        "total": len(proxies),
        "success": 0,        # 1~799ms
        "zero_or_na": 0,     # æ˜ç¡® 0ms æˆ– NA
        "timeout_fail": 0    # è¶…æ—¶æˆ–å…¶ä»–å¤±è´¥
    }

    print(f"\n[3/5] å¼€å§‹ Clash æµ‹é€Ÿï¼Œæ€»èŠ‚ç‚¹ {stats['total']} ä¸ªï¼Œå¹¶å‘ {max_workers}ï¼Œæ¯èŠ‚ç‚¹è¶…æ—¶ 45s")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_proxy = {executor.submit(clash_test_proxy_single, proxy, clash_path): proxy for proxy in proxies}

        for future in as_completed(future_to_proxy):
            proxy = future_to_proxy[future]
            delay = future.result()

            name = proxy.get("name", "Unknown")[:40]

            if delay is not None and 1 <= delay < 800:
                proxy["clash_delay"] = delay
                success_nodes.append(proxy)
                stats["success"] += 1
                with print_lock:
                    print(f"æˆåŠŸ {name.ljust(40)} â†’ {delay:3}ms")
            else:
                if delay == 0:
                    stats["zero_or_na"] += 1
                    with print_lock:
                        print(f"å¤±è´¥ {name.ljust(40)} â†’ 0ms/NA")
                else:
                    stats["timeout_fail"] += 1
                    with print_lock:
                        print(f"å¤±è´¥ {name.ljust(40)} â†’ è¶…æ—¶")

    # æŒ‰å»¶è¿Ÿæ’åº
    success_nodes.sort(key=lambda x: x.get("clash_delay", 9999))

    print("\n" + "="*70)
    print("æµ‹é€Ÿç»Ÿè®¡ç»“æœï¼š")
    print(f"æ€»èŠ‚ç‚¹          ï¼š{stats['total']}")
    print(f"æˆåŠŸ (1-799ms)  ï¼š{stats['success']} ä¸ª")
    print(f"å¤±è´¥ (0ms/NA)   ï¼š{stats['zero_or_na']} ä¸ª")
    print(f"è¶…æ—¶/å¼‚å¸¸       ï¼š{stats['timeout_fail']} ä¸ª")
    print("="*70)

    return success_nodes



async def main():
    print("=" * 60)
    print("Telegram.Node_Clash-Speedtestæµ‹è¯•ç‰ˆ V1")
    print(datetime.now(BJ_TZ).strftime("%Y-%m-%d %H:%M:%S"))
    print("=" * 60)

    preprocess_regex_rules()
    print("[1/5] åŠ è½½åŸæœ‰èŠ‚ç‚¹å’ŒæŠ“å–çŠ¶æ€")
    existing_proxies, last_message_ids = load_existing_proxies_and_state()
    print(f"å·²æœ‰èŠ‚ç‚¹æ•°: {len(existing_proxies)}")

    print("[2/5] æŠ“å– Telegram æ–°è®¢é˜…é“¾æ¥")
    urls, last_message_ids = await scrape_telegram_links(last_message_ids)
    new_proxies = []
    if urls:
        print(f"æŠ“å–åˆ° {len(urls)} ä¸ªè®¢é˜…é“¾æ¥ï¼Œå¼€å§‹ä¸‹è½½è§£æ...")
        for url in urls:
            proxies = download_and_parse(url)
            if proxies:
                new_proxies.extend(proxies)

    print(f"æ–°å¢èŠ‚ç‚¹æ•°: {len(new_proxies)}")

    all_proxies_map = {
        get_proxy_key(p): p for p in existing_proxies if is_valid_proxy(p)
    }
    added_count = 0
    for p in new_proxies:
        key = get_proxy_key(p)
        if key not in all_proxies_map:
            all_proxies_map[key] = p
            added_count += 1
    print(f"åˆå¹¶å»é‡åæ€»èŠ‚ç‚¹æ•°: {len(all_proxies_map)}ï¼Œæ–°å¢æœ‰æ•ˆèŠ‚ç‚¹: {added_count}")

    all_nodes = list(all_proxies_map.values())
    if not all_nodes:
        sys.exit("âŒ æ— ä»»ä½•èŠ‚ç‚¹å¯ç”¨ï¼Œç¨‹åºé€€å‡º")

    if ENABLE_SPEED_TEST:
        print("[3/5] ä½¿ç”¨ clash-speedtest æ ¸å¿ƒæµ‹é€Ÿ")
        clash_path = 'clash_core/clash'
        if not (os.path.isfile(clash_path) and os.access(clash_path, os.X_OK)):
            sys.exit(f"âŒ clash æ ¸å¿ƒç¼ºå¤±æˆ–ä¸å¯æ‰§è¡Œ: {clash_path}")
        # è®°å½•å¾…åˆ tested_nodes = batch_test_proxies_clash(clash_path, all_nodes, max_workers=MAX_TEST_WORKERS)
        tested_nodes = batch_test_proxies_clash(
            clash_path="clash_core/clash",   # ä½ çš„ clash å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„
            proxies=all_nodes,
            max_workers=MAX_TEST_WORKERS
            )
        success_count = len(tested_nodes)        
        fail_count = len(all_nodes) - success_count
        print(f"ğŸŒ æµ‹é€ŸæˆåŠŸèŠ‚ç‚¹æ•°: {success_count}ï¼Œå¤±è´¥èŠ‚ç‚¹æ•°: {fail_count}")        
        if not tested_nodes:
            print("âš ï¸ clashæµ‹é€Ÿå…¨éƒ¨å¤±è´¥ï¼Œå¯ç”¨å›é€€ç­–ç•¥ä¿ç•™æŒ‡å®šåœ°åŒºèŠ‚ç‚¹")
            fallback_regions = ['é¦™æ¸¯', 'æ—¥æœ¬', 'ç¾å›½', 'æ–°åŠ å¡', 'å¾·å›½']
            fallback_count = 30
            fallback_candidates = identify_regions_only(all_nodes)
            selected = []
            grouped = defaultdict(list)
            for p in fallback_candidates:
                if p.get('region_info') and p['region_info']['name'] in fallback_regions:
                    grouped[p['region_info']['name']].append(p)
            for region in fallback_regions:
                selected.extend(grouped[region][:fallback_count])
            tested_nodes = selected
        nodes_to_process = tested_nodes
    else:
        print("æµ‹é€Ÿå…³é—­ï¼Œä½¿ç”¨æ‰€æœ‰èŠ‚ç‚¹")
        nodes_to_process = all_nodes

    if not nodes_to_process:
        sys.exit("âŒ æ‰¾ä¸åˆ°ç¬¦åˆæ¡ä»¶çš„èŠ‚ç‚¹ï¼Œç¨‹åºé€€å‡º")

    print("[4/5] èŠ‚ç‚¹åœ°åŒºè¯†åˆ«å’Œé‡å‘½å")
    processed_proxies = process_proxies(nodes_to_process)

    if not processed_proxies:
        sys.exit("âŒ èŠ‚ç‚¹åœ°åŒºè¯†åˆ«å¤±è´¥ï¼Œç¨‹åºé€€å‡º")

    processed_proxies.sort(
        key=lambda p: (
            REGION_PRIORITY.index(p['region_info']['name']) if p['region_info']['name'] in REGION_PRIORITY else 99,
            p.get('clash_delay', 9999)
        )
    )
    print(f"[5/5] æ’åºå®Œæˆï¼ŒèŠ‚ç‚¹æ•°: {len(processed_proxies)}")

    final_config = generate_config(processed_proxies, last_message_ids)

    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    try:
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            yaml.dump(final_config, f, allow_unicode=True, sort_keys=False, indent=2)
        print(f"âœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜è‡³ {OUTPUT_FILE}")
        print("ğŸ‰ ä»»åŠ¡å®Œæˆï¼")
    except Exception as e:
        print(f"å†™å‡ºæ–‡ä»¶æ—¶å¼‚å¸¸: {e}")


if __name__ == "__main__":
    asyncio.run(main())
